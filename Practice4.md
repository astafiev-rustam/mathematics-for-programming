### **Теория вероятностей и статистика: от основ к практическому применению в программировании**

#### **Введение: Почему это важно?**
В мире разработки ПО и анализа данных теория вероятностей и статистика — не абстрактные математические концепции, а рабочие инструменты. Когда Netflix рекомендует вам фильм, когда банк обнаруживает мошенническую операцию, когда система автономного вождения принимает решение — за всем этим стоят вероятностные модели и статистический анализ. Рассмотрим, как эти методы применяются на практике, на конкретных примерах.

---

### **1. Основные понятия: от теории к коду**
**Вероятность** — количественная мера уверенности в наступлении события. В программировании это часто выражается через частоту возникновения определенных условий.

**Пример:** Система обработки ошибок в API:
- Всего запросов: 10,000
- Ошибок 500: 47
- Вероятность ошибки: `P = 47/10000 = 0.0047`

```python
total_requests = 10_000
errors = 47
p_error = errors / total_requests
print(f"Вероятность ошибки: {p_error:.4f}")
```

**Распределения** описывают, как вероятности распределены между возможными исходами. Для программиста ключевые типы:
- **Биномиальное:** Успехи/неудачи (например, валидные/невалидные запросы)
- **Пуассона:** Редкие события (кибератаки, hardware-сбои)
- **Нормальное:** Погрешности измерений, время отклика

**Пример:** Моделирование нагрузки на сервер (распределение Пуассона):
```python
import numpy as np
requests_per_sec = np.random.poisson(lam=5, size=1000)
print(f"Средняя нагрузка: {requests_per_sec.mean():.1f} запр./сек")
```

---

### **2. Практические примеры из разработки**

#### **Пример 1: A/B-тестирование интерфейса**
**Задача:** Сравнить конверсию двух вариантов кнопки "Купить".

- Вариант A: 45 кликов из 1000 показов (4.5%)
- Вариант B: 52 клика из 980 показов (5.3%)

**Статистический анализ:**
1. Считаем стандартную ошибку:
   ```python
   import math
   p_a, n_a = 0.045, 1000
   p_b, n_b = 0.053, 980
   
   se = math.sqrt(p_a*(1-p_a)/n_a + p_b*(1-p_b)/n_b)
   z_score = (p_b - p_a) / se
   print(f"Z-статистика: {z_score:.3f}")  # => ~1.34
   ```
2. При Z > 1.96 разница статистически значима (95% доверительный уровень). В нашем случае улучшение незначимо.

#### **Пример 2: Обнаружение аномалий в логах**
**Данные:** Время ответа API (мс):  
`[12, 15, 11, 120, 14, 13, 11, 14, 100, 15, 12]`

**Анализ:**
1. Считаем квантили:
   ```python
   data = np.array([...])
   q75, q25 = np.percentile(data, [75, 25])
   iqr = q75 - q25
   threshold = q75 + 1.5*iqr
   anomalies = data[data > threshold]  # [120, 100]
   ```
2. Реализуем автоматическое оповещение при превышении порога.

---

### **3. Вероятностные структуры данных**

#### **Bloom Filter: Эффективная проверка наличия**
**Принцип работы:**
1. Добавляем элемент через несколько хэш-функций
2. При проверке возможны ложные срабатывания (но не пропуски)

**Код:**
```python
from pybloom_live import ScalableBloomFilter
filter = ScalableBloomFilter(initial_capacity=1000)
for word in ["error", "warning", "info"]:
    filter.add(word)

print("error" in filter)  # True
print("critical" in filter)  # False (возможен False Positive)
```

**Применение:** 
- Проверка запрещённых паролей 
- Предфильтрация запросов к кэшу

---

### **4. Задачи для самостоятельной работы**

#### **Задача 1: Анализ отказов сервера**
Данные:
- Среднее время между отказами (MTBF): 120 часов
- Время восстановления (MTTR): 4 часа

**Вопросы:**
1. Рассчитайте доступность (availability) системы
2. Как изменится доступность при добавлении резервного сервера?
3. Напишите симулятор на Python (используйте экспоненциальное распределение)

#### **Задача 2: Оптимизация кэширования**
Статистика запросов:
- Всего: 1,000,000
- Уникальных: 20,000
- Частота появления топ-100 ключей: 85%

**Задание:**
1. Рассчитайте вероятность попадания в кэш размера 100
2. Предложите стратегию инвалидации
3. Сравните LRU и LFU на синтетических данных

#### **Задача 3: Доверительные интервалы для метрик**
Данные конверсии:
- День 1: 102/2000 (5.1%)
- День 2: 98/2100 (4.7%)
- ...
- День 7: 110/1950 (5.6%)

**Требуется:**
1. Рассчитать 95% доверительный интервал для средней конверсии
2. Определить, есть ли статистически значимый тренд
3. Визуализировать результат (matplotlib/seaborn)

---

### **5. Продвинутые техники**

#### **Байесовские методы для спам-фильтров**
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

emails = ["купите сейчас", "отчёт по проекту", ...]
labels = [1, 0, ...]  # 1=спам

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(emails)

model = MultinomialNB()
model.fit(X, labels)

test = vectorizer.transform(["акция сегодня"])
print(model.predict_proba(test))  # [[0.12 0.88]]
```

#### **Марковские цепи для предсказания**
Моделирование поведения пользователя на сайте:
```python
transition_matrix = {
    'main': {'main': 0.6, 'cart': 0.3, 'exit': 0.1},
    'cart': {'main': 0.1, 'cart': 0.5, 'checkout': 0.4},
    ...
}

def simulate_path(start, steps):
    path = [start]
    for _ in range(steps):
        next_state = np.random.choice(
            list(transition_matrix[path[-1]].keys()),
            p=list(transition_matrix[path[-1]].values())
        )
        path.append(next_state)
    return path
```

---

### **Заключение: Интеграция в разработку**
1. **Мониторинг:** Статистические границы для алертов
2. **Тестирование:** Определение достаточного размера выборки
3. **ML:** Оценка качества моделей (precision/recall)
4. **Безопасность:** Обнаружение аномалий

**Инструменты:**
- Python: `scipy.stats`, `statsmodels`, `pandas`
- Базы данных: оконные функции, перцентили
- Веб: Google Analytics, A/B-тесты

Теория вероятностей и статистика — это не просто математика, а язык, на котором говорят данные. Освоив его, разработчик получает возможность не только реагировать на проблемы, но и предвосхищать их, строя более надёжные и эффективные системы.
