## **1. Введение в теорию информации**  
**Теория информации** — это наука, изучающая процессы хранения, передачи и обработки информации. Она была основана Клодом Шенноном в 1948 году и применяется в криптографии, сжатии данных, телекоммуникациях и машинном обучении.  

**Основные вопросы:**  
- Как измерить информацию?  
- Как эффективно кодировать данные?  
- Как бороться с шумами при передаче?  

---

## **2. Основные понятия**  
### **2.1. Количество информации**  
**Информация** — это мера уменьшения неопределенности.  

**Формула Шеннона для количества информации:**  
Если событие имеет вероятность `p`, то количество информации `I` в его наступлении:  
\[
I = -\log_2 p
\]  

**Пример:**  
- Монета: вероятность "орла" = 0,5 → `I = -log₂(0.5) = 1 бит`.  
- Событие с вероятностью 0,125 (1/8) → `I = -log₂(0.125) = 3 бита`.  

### **2.2. Энтропия (мера неопределенности)**  
**Энтропия** — среднее количество информации, необходимое для описания случайной величины.  

Формула для ансамбля событий с вероятностями \( p_1, p_2, \dots, p_n \):  
\[
H = -\sum_{i=1}^n p_i \log_2 p_i
\]  

**Пример:**  
- Для честной монеты:  
  \[
  H = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ бит}
  \]  
- Для нечестной монеты (P(орёл) = 0.8, P(решка) = 0.2):  
  \[
  H = - (0.8 \log_2 0.8 + 0.2 \log_2 0.2) \approx 0.72 \text{ бита}
  \]  

### **2.3. Избыточность информации**  
**Избыточность** показывает, насколько сообщение можно сжать без потери информации.  

\[
R = 1 - \frac{H}{H_{max}}
\]  
где \( H_{max} = \log_2 n \) — максимальная энтропия (при равных вероятностях).  

**Пример:**  
- Для русского языка \( H \approx 4.35 \) бит/символ (из-за частот букв), а \( H_{max} = \log_2 33 \approx 5.04 \) (33 буквы).  
- Тогда избыточность:  
  \[
  R \approx 1 - \frac{4.35}{5.04} \approx 0.14 \text{ (14%)}
  \]  

### **2.4. Кодирование информации**  
**Оптимальное кодирование** минимизирует длину сообщения (например, код Хаффмана).  

**Пример:**  
Дан алфавит `{A, B, C, D}` с вероятностями:  
- `A: 0.5`, `B: 0.3`, `C: 0.1`, `D: 0.1`  

Код Хаффмана:  
- `A: 0`, `B: 10`, `C: 110`, `D: 111`  
Средняя длина кода:  
\[
L = 0.5 \times 1 + 0.3 \times 2 + 0.1 \times 3 + 0.1 \times 3 = 1.7 \text{ бит/символ}
\]  

---

## **3. Практическое задание**  
**Задача 1 (Расчёт количества информации)**  
1. Какой объем информации несет сообщение "выпало число 3" при броске шестигранного кубика?  
2. Чему равна энтропия для кубика с неравными вероятностями:  
   - `P(1) = 0.5`, `P(2) = P(3) = P(4) = P(5) = P(6) = 0.1`?  

**Задача 2 (Кодирование)**  
Постройте код Хаффмана для алфавита `{X, Y, Z, W}` с вероятностями:  
- `X: 0.4`, `Y: 0.3`, `Z: 0.2`, `W: 0.1`  
Вычислите среднюю длину кода и сравните с энтропией.  

**Задача 3 (Избыточность)**  
Для языка, где буквы `A, B, C` встречаются с вероятностями `0.7, 0.2, 0.1`, вычислите:  
1. Энтропию.  
2. Максимальную энтропию.  
3. Избыточность.  

---

## **4. Заключение**  
Теория информации позволяет оптимизировать передачу и хранение данных. Основные выводы:  
- Информация измеряется в битах и зависит от вероятности.  
- Энтропия характеризует неопределенность системы.  
- Кодирование (например, Хаффмана) помогает сжимать данные.  

**Дополнительные темы для изучения:**  
- Пропускная способность канала.  
- Помехоустойчивые коды (код Хэмминга).  
- Применение в машинном обучении (кросс-энтропия).  

**Рекомендуемая литература:**  
- Клод Шеннон, "Математическая теория связи".  
- Томас Кавер, "Элементы теории информации".  
